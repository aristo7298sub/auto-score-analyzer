from fastapi import APIRouter, UploadFile, File, HTTPException, Query, Body, Depends, status
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from sqlalchemy.orm import Session
from typing import List, Dict
from pydantic import BaseModel
import os
import logging
import tempfile
import io
from pathlib import Path
from urllib.parse import quote
from datetime import datetime
from app.services.file_service import FileService
from app.models.score import StudentScore, ScoreResponse
from app.services.analysis_service import AnalysisService
from app.services.storage_service import StorageService
from app.services.file_storage_service import file_storage
from app.services.export_service import ExportService
from app.services.visualization_service import VisualizationService
from app.core.database import get_db
from app.core.security import get_current_user, check_quota
from app.models.user import User, AnalysisLog, QuotaTransaction, ScoreFile
import pandas as pd
import uuid

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()
storage_service = StorageService()
export_service = ExportService()
visualization_service = VisualizationService()

# æœ¬åœ°æ¨¡å¼æ—¶åˆ›å»ºå¿…è¦çš„ç›®å½•
if file_storage.storage_type == "local":
    os.makedirs("uploads", exist_ok=True)
    os.makedirs("exports", exist_ok=True)
    os.makedirs("static/charts", exist_ok=True)

@router.post("/upload", response_model=ScoreResponse)
async def upload_file(
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    ä¸Šä¼ æˆç»©æ–‡ä»¶ï¼ˆéœ€è¦è®¤è¯ï¼‰
    - æ£€æŸ¥ç”¨æˆ·é…é¢
    - è®°å½•ä¸Šä¼ å’Œåˆ†ææ—¥å¿—
    - æ‰£é™¤é…é¢
    """
    start_time = datetime.utcnow()
    analysis_log = None
    
    try:
        logger.info(f"ç”¨æˆ· {current_user.username} å¼€å§‹å¤„ç†æ–‡ä»¶ä¸Šä¼ : {file.filename}")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not file.filename.endswith(('.xlsx', '.docx', '.pptx')):
            logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file.filename}")
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒ .xlsx, .docx, .pptx æ ¼å¼")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°å­˜å‚¨æœåŠ¡
        logger.info(f"ä¿å­˜æ–‡ä»¶åˆ°å­˜å‚¨: {file.filename}")
        try:
            # ä¿å­˜åˆ°äº‘å­˜å‚¨æˆ–æœ¬åœ°
            file_url = await file_storage.save_file(
                file_content=content,
                filename=file.filename,
                file_type="upload",
                content_type=file.content_type
            )
            logger.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_url}")
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¤„ç†ï¼ˆå› ä¸ºpandas/docxéœ€è¦æ–‡ä»¶è·¯å¾„ï¼‰
            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as temp_file:
                temp_file.write(content)
                file_path = temp_file.name
            logger.info(f"åˆ›å»ºä¸´æ—¶æ–‡ä»¶: {file_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise HTTPException(status_code=500, detail=f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
            logger.info("å¼€å§‹å¤„ç†æ–‡ä»¶å†…å®¹")
            if file.filename.endswith('.xlsx'):
                logger.info("å¤„ç†Excelæ–‡ä»¶")
                student_scores = await FileService.process_excel(file_path)
            elif file.filename.endswith('.docx'):
                logger.info("å¤„ç†Wordæ–‡ä»¶")
                student_scores = await FileService.process_word(file_path)
            elif file.filename.endswith('.pptx'):
                logger.info("å¤„ç†PPTæ–‡ä»¶")
                student_scores = await FileService.process_ppt(file_path)
            
            if not student_scores:
                logger.error("æœªèƒ½ä»æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆçš„æˆç»©æ•°æ®")
                raise HTTPException(status_code=400, detail="æœªèƒ½ä»æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆçš„æˆç»©æ•°æ®")
            
            student_count = len(student_scores)
            logger.info(f"âœ… æ•°æ®è§£æå®Œæˆï¼æˆåŠŸæå–åˆ° {student_count} ä¸ªå­¦ç”Ÿçš„æˆç»©æ•°æ®")
            
            # æ£€æŸ¥é…é¢ï¼ˆ1ä¸ªå­¦ç”Ÿ = 1é…é¢ï¼‰
            quota_cost = student_count
            if not check_quota(current_user, quota_cost):
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail=f"é…é¢ä¸è¶³ã€‚éœ€è¦ {quota_cost} é…é¢ï¼Œå½“å‰ä½™é¢ {current_user.quota_balance}"
                )
            
            # åˆ›å»ºåˆ†ææ—¥å¿—
            analysis_log = AnalysisLog(
                user_id=current_user.id,
                filename=file.filename,
                file_type=Path(file.filename).suffix,
                student_count=student_count,
                quota_cost=quota_cost,
                status="processing"
            )
            db.add(analysis_log)
            db.commit()
            db.refresh(analysis_log)
            
            # ä½¿ç”¨æ‰¹é‡å¹¶å‘åˆ†ææˆç»©
            logger.info(f"ğŸ” å¼€å§‹æ™ºèƒ½åˆ†æ {student_count} åå­¦ç”Ÿçš„æˆç»©ï¼ˆæœ€å¤š50ä¸ªå¹¶å‘ï¼‰...")
            
            student_scores = await AnalysisService.analyze_scores_batch(student_scores, max_concurrent=50)
            
            logger.info(f"âœ… æˆç»©åˆ†æå®Œæˆï¼å·²ä¸º {student_count} åå­¦ç”Ÿç”Ÿæˆä¸ªæ€§åŒ–åˆ†ææŠ¥å‘Š")
            
            # ä¿å­˜æˆç»©æ•°æ®
            logger.info("ğŸ’¾ ä¿å­˜æˆç»©æ•°æ®...")
            storage_service.save_scores(student_scores)
            
            # æ‰£é™¤é…é¢ï¼ˆä»…æ™®é€šç”¨æˆ·ï¼‰
            if not current_user.is_vip:
                current_user.quota_balance -= quota_cost
            current_user.quota_used += quota_cost
            
            # è®°å½•é…é¢äº¤æ˜“
            quota_transaction = QuotaTransaction(
                user_id=current_user.id,
                transaction_type="analysis_cost",
                amount=-quota_cost,
                balance_after=current_user.quota_balance,
                description=f"åˆ†ææ–‡ä»¶: {file.filename}"
            )
            db.add(quota_transaction)
            
            # æ›´æ–°åˆ†ææ—¥å¿—çŠ¶æ€
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            analysis_log.status = "success"
            analysis_log.processing_time = processing_time
            
            # ä¿å­˜ScoreFileè®°å½•ï¼ˆåŒ…å«å®Œæ•´çš„åˆ†æç»“æœJSONï¼‰
            import json
            analysis_result_json = json.dumps([score.dict() for score in student_scores], ensure_ascii=False)
            
            score_file = ScoreFile(
                user_id=current_user.id,
                filename=file.filename,
                file_size=len(content),
                file_type=Path(file.filename).suffix.lstrip('.'),  # å»æ‰å¼€å¤´çš„ç‚¹
                student_count=student_count,
                file_url=file_url,
                analysis_completed=True,
                analysis_result=analysis_result_json,
                analyzed_at=datetime.utcnow()
            )
            db.add(score_file)
            
            db.commit()
            
            logger.info("ğŸ‰ æ–‡ä»¶å¤„ç†å®Œæˆ")
            
            response_data = ScoreResponse(
                success=True,
                message="æ–‡ä»¶å¤„ç†æˆåŠŸ",
                data=student_scores,
                original_filename=file.filename,
                processing_info={
                    "student_count": student_count,
                    "analyzed_count": student_count,
                    "quota_cost": quota_cost,
                    "quota_remaining": current_user.quota_balance,
                    "processing_time": processing_time,
                    "stages_completed": ["upload", "parse", "analyze", "save"]
                }
            )
            
            logger.info(f"ğŸ“¤ å‡†å¤‡è¿”å›å“åº” - success: {response_data.success}")
            logger.info(f"ğŸ“¤ å“åº”ä¸­çš„å­¦ç”Ÿæ•°é‡: {len(response_data.data) if response_data.data else 0}")
            logger.info(f"ğŸ“¤ å“åº”processing_info: {response_data.processing_info}")
            
            return response_data

        except HTTPException as he:
            # Propagate expected HTTP errors (e.g. quota insufficient) without masking them as 500.
            logger.error(f"âŒ HTTPå¼‚å¸¸: {he.status_code} - {he.detail}")
            raise he

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}")
            logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {repr(e)}")
            import traceback
            logger.error(f"âŒ å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            
            # è®°å½•å¤±è´¥æ—¥å¿—
            if analysis_log:
                # Make sure the session is usable even if an earlier commit failed.
                try:
                    db.rollback()
                except Exception:
                    pass
                processing_time = (datetime.utcnow() - start_time).total_seconds()
                analysis_log.status = "failed"
                analysis_log.error_message = str(e)
                analysis_log.processing_time = processing_time
                try:
                    db.commit()
                except Exception:
                    db.rollback()
            
            raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logger.info("ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
            except Exception as e:
                logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    except HTTPException as he:
        logger.error(f"âŒ HTTPå¼‚å¸¸: {he.status_code} - {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"âŒ ä¸Šä¼ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        logger.error(f"âŒ å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/student/{student_name}", response_model=ScoreResponse)
async def get_student_score(student_name: str):
    """æ ¹æ®å­¦ç”Ÿå§“åæŸ¥è¯¢æˆç»©"""
    try:
        score = storage_service.get_student_score(student_name)
        if score:
            return ScoreResponse(
                success=True,
                message="æŸ¥è¯¢æˆåŠŸ",
                data=[score]
            )
        else:
            return ScoreResponse(
                success=False,
                message=f"æœªæ‰¾åˆ°å­¦ç”Ÿ {student_name} çš„æˆç»©è®°å½•"
            )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/search", response_model=ScoreResponse)
async def search_students(keyword: str = Query(..., description="æœç´¢å…³é”®è¯")):
    """æ ¹æ®å…³é”®è¯æœç´¢å­¦ç”Ÿæˆç»©"""
    try:
        scores = storage_service.search_students(keyword)
        return ScoreResponse(
            success=True,
            message="æœç´¢æˆåŠŸ",
            data=scores
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/export/{format}")
async def export_scores(
    format: str,
    request: Dict = Body(...)
):
    """
    å¯¼å‡ºæˆç»©æ•°æ® - ç›´æ¥è¿”å›æ–‡ä»¶æµ
    
    Args:
        format: å¯¼å‡ºæ ¼å¼ (xlsx æˆ– docx)
        request: åŒ…å«æˆç»©åˆ—è¡¨å’ŒåŸå§‹æ–‡ä»¶åçš„è¯·æ±‚ä½“
    """
    try:
        logger.info(f"æ”¶åˆ°å¯¼å‡ºè¯·æ±‚ï¼Œæ ¼å¼: {format}")
        logger.info(f"è¯·æ±‚ä½“ç±»å‹: {type(request)}")
        logger.info(f"è¯·æ±‚ä½“å†…å®¹: {request}")
        
        # æå–æ•°æ®
        scores_data = request.get('scores', [])
        original_filename = request.get('original_filename', '')
        
        logger.info(f"scores æ•°é‡: {len(scores_data)}")
        
        # å°†å­—å…¸è½¬æ¢ä¸º StudentScore å¯¹è±¡
        scores = []
        for score_dict in scores_data:
            try:
                scores.append(StudentScore(**score_dict))
            except Exception as e:
                logger.error(f"è½¬æ¢ StudentScore å¤±è´¥: {e}, æ•°æ®: {score_dict}")
                raise
        
        # ç”Ÿæˆæ–‡ä»¶å
        base_name = original_filename.rsplit('.', 1)[0] if original_filename else "æˆç»©åˆ†ææŠ¥å‘Š"
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        
        if format == "xlsx":
            filename = f"{base_name}-æˆç»©åˆ†æ_{timestamp}_{unique_id}.xlsx"
            media_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        elif format == "docx":
            filename = f"{base_name}-æˆç»©åˆ†æ_{timestamp}_{unique_id}.docx"
            media_type = "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¯¼å‡º
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{format}") as temp_file:
            temp_path = temp_file.name
        
        try:
            # å¯¼å‡ºåˆ°ä¸´æ—¶æ–‡ä»¶
            if format == "xlsx":
                await export_service.export_to_excel(scores, temp_path, original_filename)
            else:
                await export_service.export_to_word(scores, temp_path, original_filename)
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(temp_path, "rb") as f:
                file_content = f.read()
            
            logger.info(f"å¯¼å‡ºæ–‡ä»¶ç”ŸæˆæˆåŠŸ: {filename}, å¤§å°: {len(file_content)} bytes")
            
            # åŒæ—¶ä¿å­˜åˆ° Azure Storageï¼ˆå¤‡ä»½ï¼‰
            try:
                file_url = await file_storage.save_file(
                    file_content=file_content,
                    filename=filename,
                    file_type="export",
                    content_type=media_type
                )
                logger.info(f"æ–‡ä»¶å·²å¤‡ä»½åˆ°å­˜å‚¨: {file_url}")
            except Exception as storage_error:
                logger.warning(f"å­˜å‚¨å¤‡ä»½å¤±è´¥ï¼ˆä¸å½±å“ä¸‹è½½ï¼‰: {storage_error}")
            
            # ç›´æ¥è¿”å›æ–‡ä»¶æµç»™å‰ç«¯
            # å¯¹æ–‡ä»¶åè¿›è¡Œ URL ç¼–ç ä»¥æ”¯æŒä¸­æ–‡
            encoded_filename = quote(filename.encode('utf-8'))
            return StreamingResponse(
                io.BytesIO(file_content),
                media_type=media_type,
                headers={
                    "Content-Disposition": f'attachment; filename*=UTF-8\'\'{encoded_filename}',
                    "Content-Length": str(len(file_content))
                }
            )
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_path):
                os.remove(temp_path)
                
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")

@router.get("/charts", response_model=Dict[str, str])
async def get_charts():
    """è·å–æ‰€æœ‰å›¾è¡¨"""
    try:
        charts = await visualization_service.get_all_charts()
        return charts
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/charts/{chart_type}")
async def get_chart(chart_type: str):
    """è·å–æŒ‡å®šç±»å‹çš„å›¾è¡¨"""
    try:
        if chart_type == "score_distribution":
            file_url = await visualization_service.generate_score_distribution()
        elif chart_type == "category_pie":
            file_url = await visualization_service.generate_category_pie()
        elif chart_type == "student_comparison":
            file_url = await visualization_service.generate_student_comparison()
        elif chart_type == "question_heatmap":
            file_url = await visualization_service.generate_question_heatmap()
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„å›¾è¡¨ç±»å‹")
        
        # è¿”å›å›¾è¡¨ URLï¼ˆå¯èƒ½æ˜¯æœ¬åœ°è·¯å¾„æˆ– Azure Blob URLï¼‰
        return JSONResponse({
            "success": True,
            "chart_url": file_url,
            "chart_type": chart_type
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ==================== å†å²è®°å½•ç›¸å…³ API ====================

@router.get("/files")
async def get_user_files(
    page: int = Query(1, ge=1),
    page_size: int = Query(10, ge=1, le=100),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    è·å–å½“å‰ç”¨æˆ·çš„å†å²æ–‡ä»¶åˆ—è¡¨ï¼ˆåˆ†é¡µï¼‰
    """
    try:
        # è®¡ç®—åç§»é‡
        offset = (page - 1) * page_size
        
        # æŸ¥è¯¢æ€»æ•°
        total = db.query(ScoreFile).filter(ScoreFile.user_id == current_user.id).count()
        
        # æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨ï¼ˆæŒ‰ä¸Šä¼ æ—¶é—´å€’åºï¼‰
        files = db.query(ScoreFile).filter(
            ScoreFile.user_id == current_user.id
        ).order_by(
            ScoreFile.uploaded_at.desc()
        ).offset(offset).limit(page_size).all()
        
        # æ ¼å¼åŒ–è¿”å›æ•°æ®
        file_list = []
        for file in files:
            file_list.append({
                "id": file.id,
                "filename": file.filename,
                "file_type": file.file_type,
                "file_size": file.file_size,
                "student_count": file.student_count,
                "analysis_completed": file.analysis_completed,
                "uploaded_at": file.uploaded_at.isoformat() if file.uploaded_at else None,
                "analyzed_at": file.analyzed_at.isoformat() if file.analyzed_at else None,
            })
        
        return JSONResponse({
            "success": True,
            "data": file_list,
            "pagination": {
                "page": page,
                "page_size": page_size,
                "total": total,
                "total_pages": (total + page_size - 1) // page_size
            }
        })
    except Exception as e:
        logger.error(f"è·å–å†å²æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å†å²æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/files/{file_id}")
async def get_file_detail(
    file_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    è·å–æŒ‡å®šæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬å­¦ç”Ÿæˆç»©æ•°æ®ï¼‰
    """
    try:
        # æŸ¥è¯¢æ–‡ä»¶è®°å½•
        file_record = db.query(ScoreFile).filter(
            ScoreFile.id == file_id,
            ScoreFile.user_id == current_user.id
        ).first()
        
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è§£æä¿å­˜çš„åˆ†æç»“æœJSON
        import json
        students_data = []
        if file_record.analysis_result:
            try:
                students_data = json.loads(file_record.analysis_result)
            except Exception as e:
                logger.warning(f"è§£æåˆ†æç»“æœå¤±è´¥: {str(e)}")
        
        return JSONResponse({
            "success": True,
            "data": {
                "id": file_record.id,
                "filename": file_record.filename,
                "file_type": file_record.file_type,
                "file_size": file_record.file_size,
                "file_url": file_record.file_url,
                "student_count": file_record.student_count,
                "analysis_completed": file_record.analysis_completed,
                "students": students_data,
                "uploaded_at": file_record.uploaded_at.isoformat() if file_record.uploaded_at else None,
                "analyzed_at": file_record.analyzed_at.isoformat() if file_record.analyzed_at else None,
                # "students": []  # TODO: æ·»åŠ å­¦ç”Ÿæˆç»©æ•°æ®
            }
        })
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶è¯¦æƒ…å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶è¯¦æƒ…å¤±è´¥: {str(e)}")


@router.delete("/files/{file_id}")
async def delete_file(
    file_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    åˆ é™¤æŒ‡å®šçš„å†å²æ–‡ä»¶è®°å½•
    """
    try:
        # æŸ¥è¯¢æ–‡ä»¶è®°å½•
        file_record = db.query(ScoreFile).filter(
            ScoreFile.id == file_id,
            ScoreFile.user_id == current_user.id
        ).first()
        
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åˆ é™¤äº‘å­˜å‚¨ä¸­çš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        if file_record.file_url:
            try:
                # ä» URL ä¸­æå– blob åç§°
                blob_name = file_record.file_url.split('/')[-1] if '/' in file_record.file_url else file_record.file_url
                await file_storage.delete_file(blob_name, file_type="upload")
                logger.info(f"å·²åˆ é™¤äº‘å­˜å‚¨æ–‡ä»¶: {blob_name}")
            except Exception as e:
                logger.warning(f"åˆ é™¤äº‘å­˜å‚¨æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # åˆ é™¤æ•°æ®åº“è®°å½•
        db.delete(file_record)
        db.commit()
        
        return JSONResponse({
            "success": True,
            "message": "æ–‡ä»¶åˆ é™¤æˆåŠŸ"
        })
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.post("/files/batch-delete")
async def batch_delete_files(
    file_ids: List[int] = Body(..., embed=True),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    æ‰¹é‡åˆ é™¤å†å²æ–‡ä»¶è®°å½•
    """
    try:
        deleted_count = 0
        failed_ids = []
        
        for file_id in file_ids:
            try:
                # æŸ¥è¯¢æ–‡ä»¶è®°å½•
                file_record = db.query(ScoreFile).filter(
                    ScoreFile.id == file_id,
                    ScoreFile.user_id == current_user.id
                ).first()
                
                if file_record:
                    # åˆ é™¤äº‘å­˜å‚¨ä¸­çš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
                    if file_record.file_url:
                        try:
                            # ä» URL ä¸­æå– blob åç§°
                            blob_name = file_record.file_url.split('/')[-1] if '/' in file_record.file_url else file_record.file_url
                            await file_storage.delete_file(blob_name, file_type="upload")
                            logger.info(f"å·²åˆ é™¤äº‘å­˜å‚¨æ–‡ä»¶: {blob_name}")
                        except Exception as e:
                            logger.warning(f"åˆ é™¤äº‘å­˜å‚¨æ–‡ä»¶å¤±è´¥: {str(e)}")
                    
                    # åˆ é™¤æ•°æ®åº“è®°å½•
                    db.delete(file_record)
                    deleted_count += 1
                else:
                    failed_ids.append(file_id)
            except Exception as e:
                logger.error(f"åˆ é™¤æ–‡ä»¶ {file_id} å¤±è´¥: {str(e)}")
                failed_ids.append(file_id)
        
        db.commit()
        
        return JSONResponse({
            "success": True,
            "message": f"æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶",
            "deleted_count": deleted_count,
            "failed_ids": failed_ids
        })
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}") 